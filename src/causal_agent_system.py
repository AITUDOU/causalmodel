"""
å› æœé©±åŠ¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿ - åŸºäº LangGraph
ä¸‰æ™ºèƒ½ä½“æ¶æ„ï¼šRouter Agent â†’ Causal Analyst Agent â†’ Advisor Agent

åº”ç”¨åœºæ™¯ï¼šæ··å‡åœŸé›†æ–™è´¨é‡æ§åˆ¶ä¸å·¥è‰ºä¼˜åŒ–
"""

import os
import pickle
from pathlib import Path
from typing import TypedDict, Literal, Dict, Any, Optional
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
import pandas as pd
import numpy as np
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥æ··å‡åœŸé›†æ–™å› æœæ¨¡å‹
from causal_model import ConcreteAggregateCausalModel


# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_API_BASE = os.getenv('OPENAI_API_BASE', 'https://api.openai.com/v1')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')


# ============================================================================
# ç¬¬ä¸€æ­¥ï¼šå®šä¹‰ State
# ============================================================================

class CausalAnalysisState(TypedDict):
    """å› æœåˆ†ææ™ºèƒ½ä½“ç³»ç»Ÿçš„å…±äº«çŠ¶æ€"""
    # ç”¨æˆ·è¾“å…¥
    user_query: str                    # åŸå§‹æŸ¥è¯¢
    reference_sample_index: int        # å‚è€ƒæ‰¹æ¬¡ç´¢å¼•ï¼ˆç”¨äºåäº‹å®åˆ†æï¼‰
    
    # Router è¾“å‡º
    analysis_type: str                 # 'attribution' | 'intervention' | 'counterfactual'
    target_variable: str               # ç›®æ ‡å˜é‡
    intervention_params: dict          # å¹²é¢„å‚æ•°
    routing_reasoning: str             # è·¯ç”±æ¨ç†è¿‡ç¨‹
    
    # Causal Analyst è¾“å‡º
    causal_results: dict               # å› æœåˆ†ææ•°å€¼ç»“æœ
    analysis_summary: str              # åˆ†ææ‘˜è¦
    
    # Advisor è¾“å‡º
    recommendations: str               # å†³ç­–å»ºè®®
    
    # ç³»ç»Ÿå…ƒæ•°æ®
    error: Optional[str]               # é”™è¯¯ä¿¡æ¯


# ============================================================================
# ç¬¬äºŒæ­¥ï¼šå®šä¹‰å·¥å…·å‡½æ•° - åŒ…è£…ç°æœ‰å› æœæ¨¡å‹
# ============================================================================

# å…¨å±€å› æœæ¨¡å‹å®ä¾‹
_causal_model_instance: Optional[ConcreteAggregateCausalModel] = None

# æ¨¡å‹ç¼“å­˜è·¯å¾„
MODEL_CACHE_FILE = Path("models/causal_model.pkl")


def initialize_causal_model(df: pd.DataFrame = None, force_retrain: bool = False) -> ConcreteAggregateCausalModel:
    """
    åˆå§‹åŒ–æ··å‡åœŸé›†æ–™å› æœæ¨¡å‹ï¼ˆæ”¯æŒç¼“å­˜åŠ è½½ï¼‰
    
    Args:
        df: æ··å‡åœŸé›†æ–™æ•°æ®ï¼ˆå¦‚æœä»ç¼“å­˜åŠ è½½åˆ™å¯ä»¥ä¸º Noneï¼‰
        force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼ˆé»˜è®¤ Falseï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰
        
    Returns:
        åˆå§‹åŒ–å¥½çš„å› æœæ¨¡å‹
    """
    global _causal_model_instance
    
    # ä¼˜å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
    if not force_retrain and MODEL_CACHE_FILE.exists():
        print("ğŸ“¦ ä»ç¼“å­˜åŠ è½½å› æœæ¨¡å‹...")
        try:
            with open(MODEL_CACHE_FILE, 'rb') as f:
                model = pickle.load(f)
            _causal_model_instance = model
            print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (ç¼“å­˜æ–‡ä»¶: {MODEL_CACHE_FILE})")
            print(f"  â€¢ èŠ‚ç‚¹æ•°: {model.causal_graph.number_of_nodes()}")
            print(f"  â€¢ è¾¹æ•°: {model.causal_graph.number_of_edges()}")
            return model
        except Exception as e:
            print(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            print("   å°†é‡æ–°è®­ç»ƒæ¨¡å‹...")
    
    # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œé‡æ–°è®­ç»ƒ
    if df is None:
        raise ValueError(
            "æœªæ‰¾åˆ°ç¼“å­˜æ¨¡å‹ä¸”æœªæä¾›æ•°æ®ã€‚è¯·å…ˆè¿è¡Œ train_causal_model.py è®­ç»ƒæ¨¡å‹ï¼Œ"
            "æˆ–è€…åœ¨è°ƒç”¨æ—¶æä¾›æ•°æ®ã€‚"
        )
    
    print("ğŸ”§ è®­ç»ƒæ–°çš„å› æœæ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦1-2åˆ†é’Ÿï¼‰...")
    model = ConcreteAggregateCausalModel(df)
    model.build_causal_graph()
    model.fit_causal_model(quality='BETTER', invertible=True)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    MODEL_CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(MODEL_CACHE_FILE, 'wb') as f:
        pickle.dump(model, f)
    print(f"âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜è‡³: {MODEL_CACHE_FILE}")
    
    _causal_model_instance = model
    return model


@tool
def attribution_analysis_tool(
    target_variable: str,
    old_period_start: int,
    old_period_end: int,
    new_period_start: int,
    new_period_end: int
) -> dict:
    """
    æ‰§è¡Œå½’å› åˆ†æï¼Œè¯†åˆ«ç›®æ ‡å˜é‡å˜åŒ–çš„æ ¹æœ¬åŸå› ã€‚
    
    ç”¨äºå›ç­”"ä¸ºä»€ä¹ˆå«æ³¥é‡ä¸Šå‡äº†ï¼Ÿ"ã€"æ˜¯ä»€ä¹ˆå¯¼è‡´å¼ºåº¦ä¸‹é™ï¼Ÿ"ç­‰é—®é¢˜ã€‚
    å¯¹æ¯”ä¸¤ä¸ªæ—¶é—´æ®µçš„æ•°æ®ï¼Œæ‰¾å‡ºå“ªäº›å› ç´ å¯¹ç›®æ ‡å˜é‡çš„å˜åŒ–è´¡çŒ®æœ€å¤§ã€‚
    
    Args:
        target_variable: ç›®æ ‡å˜é‡ï¼ˆå¦‚ 'mud_content_pct', 'concrete_strength_mpa'ï¼‰
        old_period_start: æ—§æ—¶æœŸèµ·å§‹ç´¢å¼•
        old_period_end: æ—§æ—¶æœŸç»“æŸç´¢å¼•
        new_period_start: æ–°æ—¶æœŸèµ·å§‹ç´¢å¼•
        new_period_end: æ–°æ—¶æœŸç»“æŸç´¢å¼•
        
    Returns:
        dict: åŒ…å«å„å› ç´ è´¡çŒ®åº¦çš„åˆ†æç»“æœ
    """
    if _causal_model_instance is None:
        return {"error": "å› æœæ¨¡å‹æœªåˆå§‹åŒ–"}
    
    try:
        df = _causal_model_instance.df
        df_old = df.iloc[old_period_start:old_period_end]
        df_new = df.iloc[new_period_start:new_period_end]
        
        contributions, uncertainties = _causal_model_instance.attribution_analysis(
            df_old=df_old,
            df_new=df_new,
            target_column=target_variable,
            num_samples=2000,
            num_bootstrap_resamples=4
        )
        
        # æŒ‰è´¡çŒ®åº¦æ’åº
        sorted_contributions = sorted(
            contributions.items(), 
            key=lambda x: abs(x[1]), 
            reverse=True
        )
        
        # å°† numpy æ•°ç»„è½¬æ¢ä¸º Python åŸç”Ÿç±»å‹
        def convert_to_native(val):
            """è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹"""
            if val is None:
                return None
            if isinstance(val, np.ndarray):
                return val.tolist()
            if isinstance(val, (np.floating, np.integer)):
                return float(val)
            if isinstance(val, tuple):
                return tuple(convert_to_native(v) for v in val)
            return val
        
        result = {
            "type": "attribution",
            "target": target_variable,
            "top_factors": [
                {
                    "variable": var,
                    "contribution": float(contrib),
                    "confidence_interval": convert_to_native(uncertainties.get(var, (None, None)))
                }
                for var, contrib in sorted_contributions[:5]
            ],
            "old_period_size": len(df_old),
            "new_period_size": len(df_new)
        }
        
        return result
        
    except Exception as e:
        return {"error": str(e)}


@tool
def intervention_analysis_tool(
    target_variable: str,
    step_size: float = 1.0
) -> dict:
    """
    æ‰§è¡Œå¹²é¢„åˆ†æï¼Œè¯„ä¼°å„å› ç´ å¯¹ç›®æ ‡å˜é‡çš„å› æœæ•ˆåº”ã€‚
    
    ç”¨äºå›ç­”"å¦‚ä½•é™ä½å«æ³¥é‡ï¼Ÿ"ã€"å“ªäº›å·¥è‰ºå‚æ•°æœ€æœ‰æ•ˆï¼Ÿ"ç­‰é—®é¢˜ã€‚
    è®¡ç®—æ¯ä¸ªå¯æ§å˜é‡å¢åŠ ä¸€ä¸ªå•ä½åï¼Œå¯¹ç›®æ ‡å˜é‡çš„å½±å“ç¨‹åº¦ã€‚
    
    Args:
        target_variable: ç›®æ ‡å˜é‡ï¼ˆå¦‚ 'mud_content_pct', 'concrete_strength_mpa'ï¼‰
        step_size: å¹²é¢„æ­¥é•¿ï¼ˆé»˜è®¤ä¸º1.0ï¼‰
        
    Returns:
        dict: åŒ…å«å„å˜é‡å› æœæ•ˆåº”çš„åˆ†æç»“æœ
    """
    if _causal_model_instance is None:
        return {"error": "å› æœæ¨¡å‹æœªåˆå§‹åŒ–"}
    
    try:
        results_df = _causal_model_instance.intervention_analysis(
            target=target_variable,
            step_size=step_size,
            num_samples=10000,
            num_bootstrap_resamples=40
        )
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        interventions = []
        for _, row in results_df.iterrows():
            interventions.append({
                "variable": row['Variable'],
                "causal_effect": float(row['Causal_Effect']),
                "confidence_interval": (float(row['Lower_CI']), float(row['Upper_CI'])),
                "std_error": float(row.get('Std_Error', 0))
            })
        
        result = {
            "type": "intervention",
            "target": target_variable,
            "step_size": step_size,
            "interventions": interventions
        }
        
        return result
        
    except Exception as e:
        return {"error": str(e)}


@tool
def counterfactual_analysis_tool(
    sample_index: int,
    interventions: dict,
    target_variable: str
) -> dict:
    """
    æ‰§è¡Œåäº‹å®åˆ†æï¼Œé¢„æµ‹"å¦‚æœæ”¹å˜æŸäº›å˜é‡ï¼Œç»“æœä¼šå¦‚ä½•"ã€‚
    
    ç”¨äºå›ç­”"å¦‚æœæ”¹å˜æ°´èƒ¶æ¯”ä¼šæ€æ ·ï¼Ÿ"ã€"æ¢ä¸ªäº§åœ°èƒ½è¾¾æ ‡å—ï¼Ÿ"ç­‰é—®é¢˜ã€‚
    é’ˆå¯¹å…·ä½“çš„å†å²æ ·æœ¬ï¼Œæ¨¡æ‹Ÿæ”¹å˜ä¸€ä¸ªæˆ–å¤šä¸ªå˜é‡åçš„ç»“æœã€‚
    
    Args:
        sample_index: æ ·æœ¬ç´¢å¼•ï¼ˆè¦åˆ†æçš„å†å²è®°å½•ï¼‰
        interventions: å¹²é¢„å˜é‡åŠå…¶æ–°å€¼çš„å­—å…¸ï¼Œå¦‚ {"water_binder_ratio": 0.43, "cement_content": 379}
        target_variable: ç›®æ ‡å˜é‡åç§°
        
    Returns:
        dict: åŒ…å«å®é™…å€¼ã€åäº‹å®å€¼å’Œå˜åŒ–çš„ç»“æœ
    """
    if _causal_model_instance is None:
        return {"error": "å› æœæ¨¡å‹æœªåˆå§‹åŒ–"}
    
    try:
        df = _causal_model_instance.df
        observed_data = df.iloc[[sample_index]]
        
        # è½¬æ¢å¹²é¢„å€¼ä¸ºfloat
        interventions_float = {k: float(v) for k, v in interventions.items()}
        
        result_dict = _causal_model_instance.counterfactual_analysis(
            observed_data=observed_data,
            interventions=interventions_float,
            target=target_variable,
            num_samples=1000
        )
        
        # è½¬æ¢ä¸ºåŸç”Ÿ Python ç±»å‹
        def to_float(val):
            """å®‰å…¨åœ°è½¬æ¢ä¸º float"""
            if val is None:
                return None
            if isinstance(val, (np.floating, np.integer, np.ndarray)):
                return float(val)
            return float(val)
        
        # æ„å»ºå¹²é¢„ä¿¡æ¯åˆ—è¡¨
        intervention_list = []
        for var, new_val in interventions_float.items():
            original_val = to_float(observed_data[var].values[0]) if var in observed_data.columns else None
            intervention_list.append({
                "variable": var,
                "original_value": original_val,
                "new_value": float(new_val)
            })
        
        result = {
            "type": "counterfactual",
            "sample_index": sample_index,
            "target": target_variable,
            "interventions": intervention_list,  # ç°åœ¨æ˜¯åˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªå¹²é¢„
            "observed_mean": to_float(result_dict['observed_mean']),
            "counterfactual_mean": to_float(result_dict['counterfactual_mean']),
            "causal_effect": to_float(result_dict['causal_effect'])
        }
        
        return result
        
    except Exception as e:
        return {"error": str(e)}


# ============================================================================
# ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰ä¸‰ä¸ªæ™ºèƒ½ä½“èŠ‚ç‚¹
# ============================================================================

def router_agent(state: CausalAnalysisState) -> dict:
    """
    Router Agentï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«åˆ†æç±»å‹å’Œå…³é”®å‚æ•°
    
    èŒè´£ï¼š
    1. ç†è§£è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„æ„å›¾
    2. è¯†åˆ«æŸ¥è¯¢ç±»å‹ï¼ˆå½’å› /å¹²é¢„/åäº‹å®ï¼‰
    3. æå–å…³é”®ä¿¡æ¯ï¼ˆç›®æ ‡å˜é‡ã€å¹²é¢„å‚æ•°ç­‰ï¼‰
    """
    print("\n" + "="*80)
    print("ğŸ” Router Agent æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")
    print("="*80)
    
    # ä½¿ç”¨ LLM ç†è§£ç”¨æˆ·æŸ¥è¯¢
    llm = ChatOpenAI(
        model=OPENAI_MODEL,
        temperature=0.1,
        openai_api_key=OPENAI_API_KEY,
        base_url=OPENAI_API_BASE
    )
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå› æœåˆ†æç³»ç»Ÿçš„è·¯ç”±ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œç¡®å®šåº”è¯¥æ‰§è¡Œå“ªç§å› æœåˆ†æã€‚

ã€åº”ç”¨åœºæ™¯ã€‘é«˜æ€§èƒ½æ··å‡åœŸé…åˆæ¯”è®¾è®¡ä¸å¼ºåº¦ä¼˜åŒ–ï¼ˆåŸºäºUCIçœŸå®æ•°æ®é›†ï¼ŒYeh 1998ï¼‰

ã€åˆ†æç±»å‹ã€‘
1. **attribution**ï¼ˆå½’å› åˆ†æï¼‰- ç”¨äºå›ç­”"ä¸ºä»€ä¹ˆXXå˜åŒ–äº†ï¼Ÿ"ã€"åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
   - å¯¹æ¯”ä¸¤ä¸ªæ—¶æœŸçš„æ•°æ®ï¼Œè¯†åˆ«å¯¼è‡´ç›®æ ‡å˜é‡å˜åŒ–çš„æ ¹æœ¬åŸå› 

2. **intervention**ï¼ˆå¹²é¢„åˆ†æï¼‰- ç”¨äºå›ç­”"å¦‚ä½•æ”¹è¿›XXï¼Ÿ"ã€"å“ªä¸ªå› ç´ æœ€æœ‰æ•ˆï¼Ÿ"
   - è¯„ä¼°ä¸åŒæªæ–½çš„æ•ˆæœï¼Œæ‰¾å‡ºæœ€æœ‰å½±å“åŠ›çš„å¯æ§å˜é‡

3. **counterfactual**ï¼ˆåäº‹å®åˆ†æï¼‰- ç”¨äºå›ç­”"å¦‚æœ...ä¼šæ€æ ·ï¼Ÿ"
   - é’ˆå¯¹å…·ä½“æ¡ˆä¾‹æ¨¡æ‹Ÿå‡è®¾åœºæ™¯ï¼Œé¢„æµ‹æ”¹å˜æŸä¸ªå˜é‡åçš„ç»“æœ
   - **å¿…é¡»ä»ç”¨æˆ·é—®é¢˜ä¸­æå–**ï¼šå¹²é¢„å˜é‡åã€åŸå§‹å€¼ã€æ–°å€¼

ã€å› æœå›¾å¯ç”¨å˜é‡ã€‘ï¼ˆåŸºäºUCIçœŸå®æ•°æ®é›†ï¼Œä»…9ä¸ªåŸå§‹å˜é‡ï¼‰

1. cement: æ°´æ³¥ (102-540 kg/mÂ³, å‡å€¼281) **ã€å…³é”®ææ–™ã€‘**
2. blast_furnace_slag: é«˜ç‚‰çŸ¿æ¸£ (0-359 kg/mÂ³, å‡å€¼74) - æé«˜å¯†å®åº¦å’Œè€ä¹…æ€§
3. fly_ash: ç²‰ç…¤ç° (0-200 kg/mÂ³, å‡å€¼54) - ç«å±±ç°ååº”ï¼Œé•¿æœŸå¼ºåº¦
4. water: æ°´ (122-247 kg/mÂ³, å‡å€¼182) **ã€Abramså®šå¾‹ï¼šæ°´è¶Šå¤šå¼ºåº¦è¶Šä½ã€‘**
5. superplasticizer: é«˜æ•ˆå‡æ°´å‰‚ (0-32 kg/mÂ³, å‡å€¼6.2) - ä¸æ°´è´Ÿç›¸å…³ï¼ˆr=-0.66ï¼‰
6. coarse_aggregate: ç²—éª¨æ–™ (801-1145 kg/mÂ³, å‡å€¼973) - éª¨æ¶ä½œç”¨
7. fine_aggregate: ç»†éª¨æ–™ (594-993 kg/mÂ³, å‡å€¼774) - å¡«å……ä½œç”¨
8. age: é¾„æœŸ (1-365å¤©, å‡å€¼46å¤©) **ã€æ—¶é—´æ•ˆåº”ã€‘**
9. concrete_compressive_strength: æŠ—å‹å¼ºåº¦ (2.3-82.6 MPa, å‡å€¼35.8) **ã€ç›®æ ‡å˜é‡ã€‘**

ç”¨æˆ·æŸ¥è¯¢ï¼š"{state['user_query']}"

ã€å˜é‡è¯†åˆ«è§„åˆ™ã€‘
- ç”¨æˆ·æåˆ°"å¼ºåº¦"/"æŠ—å‹å¼ºåº¦"/"æ··å‡åœŸå¼ºåº¦" â†’ concrete_compressive_strength
- ç”¨æˆ·æåˆ°"æ°´æ³¥"/"æ°´æ³¥ç”¨é‡" â†’ cement
- ç”¨æˆ·æåˆ°"çŸ¿æ¸£"/"é«˜ç‚‰çŸ¿æ¸£"/"çŸ¿ç²‰" â†’ blast_furnace_slag  
- ç”¨æˆ·æåˆ°"ç²‰ç…¤ç°" â†’ fly_ash
- ç”¨æˆ·æåˆ°"æ°´"/"ç”¨æ°´é‡"/"æ‹Œåˆæ°´" â†’ water
- ç”¨æˆ·æåˆ°"å‡æ°´å‰‚"/"å¤–åŠ å‰‚"/"é«˜æ•ˆå‡æ°´å‰‚"/"è¶…å¡‘åŒ–å‰‚" â†’ superplasticizer
- ç”¨æˆ·æåˆ°"ç²—éª¨æ–™"/"çŸ³å­"/"ç¢çŸ³" â†’ coarse_aggregate
- ç”¨æˆ·æåˆ°"ç»†éª¨æ–™"/"ç ‚"/"æ²³ç ‚" â†’ fine_aggregate
- ç”¨æˆ·æåˆ°"é¾„æœŸ"/"å…»æŠ¤æ—¶é—´"/"å¤©æ•°"/"é¾„é¾„" â†’ age

æ³¨æ„ï¼šæœ¬æ¨¡å‹ä½¿ç”¨çœŸå®UCIæ•°æ®é›†çš„9ä¸ªåŸå§‹å˜é‡ï¼Œæœªä½¿ç”¨è¡ç”Ÿå˜é‡

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "analysis_type": "attribution/intervention/counterfactual",
    "target_variable": "ä»ä¸Šè¿°å˜é‡åˆ—è¡¨ä¸­é€‰æ‹©ï¼ˆå¿…é¡»æ˜¯å‡†ç¡®çš„å˜é‡åï¼‰",
    "reasoning": "ä½ çš„æ¨ç†è¿‡ç¨‹ï¼ˆ1-2å¥è¯ï¼‰",
    "extracted_info": {{
        // å¦‚æœæ˜¯åäº‹å®åˆ†æï¼š
        // å•å˜é‡å¹²é¢„ï¼š
        "intervention_variable": "å˜é‡å",
        "original_value": åŸå§‹æ•°å€¼,
        "intervention_value": æ–°æ•°å€¼
        // æˆ–è€…å¤šå˜é‡å¹²é¢„ï¼š
        "intervention_variable": {{"water": 150, "cement": 300}}
    }}
}}

ç¤ºä¾‹1ï¼ˆå•å˜é‡ï¼‰ï¼š
ç”¨æˆ·é—®ï¼š"å¦‚æœæ°´ç”¨é‡ä»200é™åˆ°150ï¼Œå¼ºåº¦ä¼šæ€æ ·ï¼Ÿ"
å›å¤ï¼š{{"intervention_variable": "water", "original_value": 200, "intervention_value": 150}}

ç¤ºä¾‹2ï¼ˆå¤šå˜é‡ï¼‰ï¼š
ç”¨æˆ·é—®ï¼š"å¦‚æœæ°´æ³¥300ã€æ°´180ã€é¾„æœŸ28å¤©ï¼Œå¼ºåº¦æ˜¯å¤šå°‘ï¼Ÿ"
å›å¤ï¼š{{"intervention_variable": {{"cement": 300, "water": 180, "age": 28}}}}
"""
    
    response = llm.invoke(prompt)
    
    # è§£æ LLM å“åº”
    import json
    try:
        # æå–JSONå†…å®¹
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
            
        parsed = json.loads(content)
        
        print(f"\nğŸ“‹ åˆ†æç±»å‹: {parsed['analysis_type']}")
        print(f"ğŸ¯ ç›®æ ‡å˜é‡: {parsed['target_variable']}")
        print(f"ğŸ’¡ æ¨ç†: {parsed['reasoning']}")
        
        return {
            "analysis_type": parsed['analysis_type'],
            "target_variable": parsed['target_variable'],
            "routing_reasoning": parsed['reasoning'],
            "intervention_params": parsed.get('extracted_info', {})
        }
        
    except Exception as e:
        print(f"âš ï¸ è·¯ç”±è§£æå¤±è´¥: {e}")
        # é»˜è®¤ä½¿ç”¨å¹²é¢„åˆ†æ
        return {
            "analysis_type": "intervention",
            "target_variable": "concrete_compressive_strength",
            "routing_reasoning": "ä½¿ç”¨é»˜è®¤é…ç½®",
            "intervention_params": {}
        }


def causal_analyst_agent(state: CausalAnalysisState) -> dict:
    """
    Causal Analyst Agentï¼šæ‰§è¡Œå› æœåˆ†æ
    
    èŒè´£ï¼š
    1. æ ¹æ® Router çš„æŒ‡ç¤ºè°ƒç”¨å¯¹åº”çš„å› æœåˆ†æå·¥å…·
    2. æ‰§è¡Œè®¡ç®—å¹¶è¿”å›å®šé‡ç»“æœ
    """
    print("\n" + "="*80)
    print("ğŸ“Š Causal Analyst Agent æ­£åœ¨æ‰§è¡Œå› æœåˆ†æ...")
    print("="*80)
    
    analysis_type = state['analysis_type']
    target_variable = state['target_variable']
    
    try:
        if analysis_type == 'attribution':
            print("æ‰§è¡Œå½’å› åˆ†æ...")
            # é»˜è®¤å¯¹æ¯”å‰300æ¡å’Œå300æ¡æ•°æ®ï¼ˆçœŸå®æ•°æ®å…±1030æ¡ï¼‰
            result = attribution_analysis_tool.invoke({
                "target_variable": target_variable,
                "old_period_start": 0,
                "old_period_end": 300,
                "new_period_start": 700,
                "new_period_end": 1000
            })
            
        elif analysis_type == 'intervention':
            print("æ‰§è¡Œå¹²é¢„åˆ†æ...")
            result = intervention_analysis_tool.invoke({
                "target_variable": target_variable,
                "step_size": 1.0
            })
            
        elif analysis_type == 'counterfactual':
            print("æ‰§è¡Œåäº‹å®åˆ†æ...")
            params = state.get('intervention_params', {})
            
            # ä» Router æå–çš„å‚æ•°ä¸­è·å–å¹²é¢„ä¿¡æ¯
            # æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªå˜é‡çš„å¹²é¢„
            intervention_variable = params.get('intervention_variable')
            intervention_value = params.get('intervention_value')
            original_value = params.get('original_value')
            
            # æ„å»ºå¹²é¢„å­—å…¸
            interventions = {}
            
            # æƒ…å†µ1ï¼šæå–åˆ°å•ä¸ªå˜é‡çš„å¹²é¢„
            if isinstance(intervention_variable, str) and intervention_value is not None:
                interventions[intervention_variable] = intervention_value
            
            # æƒ…å†µ2ï¼šæå–åˆ°å¤šä¸ªå˜é‡çš„å¹²é¢„ï¼ˆRouterå¯èƒ½æå–ä¸ºå­—å…¸ï¼‰
            elif isinstance(intervention_variable, dict):
                interventions = intervention_variable
            elif isinstance(intervention_value, dict):
                interventions = intervention_value
            
            # æƒ…å†µ3ï¼šæ²¡æœ‰æå–åˆ°å¹²é¢„ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤
            if not interventions:
                interventions = {'water': 150}  # é»˜è®¤é™ä½ç”¨æ°´é‡
                print(f"  âš ï¸  æœªæå–åˆ°å¹²é¢„ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å¹²é¢„: {interventions}")
            
            # é€‰æ‹©åˆé€‚çš„æ ·æœ¬è¿›è¡Œåˆ†æ
            # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„å‚è€ƒæ‰¹æ¬¡
            if state.get('reference_sample_index') is not None:
                sample_index = state['reference_sample_index']
                print(f"  ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„å‚è€ƒæ‰¹æ¬¡: ç´¢å¼• {sample_index}")
            # å¦‚æœæå–åˆ°äº†åŸå§‹å€¼ï¼Œå°è¯•æ‰¾åˆ°æ¥è¿‘è¯¥å€¼çš„æ ·æœ¬
            elif original_value is not None and _causal_model_instance is not None:
                df = _causal_model_instance.df
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¹²é¢„å˜é‡æ‰¾åˆ°æœ€æ¥è¿‘çš„æ ·æœ¬
                first_var = list(interventions.keys())[0]
                if first_var in df.columns:
                    closest_idx = (df[first_var] - original_value).abs().idxmin()
                    sample_index = int(closest_idx)
                    print(f"  æ‰¾åˆ°æœ€æ¥è¿‘åŸå§‹å€¼ {original_value} çš„æ ·æœ¬: ç´¢å¼• {sample_index}")
            else:
                sample_index = 100  # é»˜è®¤æ ·æœ¬
                print(f"  ä½¿ç”¨é»˜è®¤æ ·æœ¬ç´¢å¼•: {sample_index}")
            
            print(f"  å¹²é¢„å˜é‡: {', '.join(interventions.keys())}")
            print(f"  å¹²é¢„å€¼: {interventions}")
            
            result = counterfactual_analysis_tool.invoke({
                "sample_index": sample_index,
                "interventions": interventions,
                "target_variable": target_variable
            })
            
        else:
            result = {"error": f"æœªçŸ¥çš„åˆ†æç±»å‹: {analysis_type}"}
        
        if "error" in result:
            print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
            return {
                "causal_results": result,
                "analysis_summary": f"åˆ†æå¤±è´¥: {result['error']}",
                "error": result['error']
            }
        
        # ç”Ÿæˆç®€è¦æ‘˜è¦
        summary = _generate_analysis_summary(result)
        print(f"\nâœ“ åˆ†æå®Œæˆ")
        print(f"ğŸ“ æ‘˜è¦: {summary[:200]}...")
        
        return {
            "causal_results": result,
            "analysis_summary": summary
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æå¼‚å¸¸: {e}")
        return {
            "causal_results": {"error": str(e)},
            "analysis_summary": f"åˆ†æå¼‚å¸¸: {e}",
            "error": str(e)
        }


def advisor_agent(state: CausalAnalysisState) -> dict:
    """
    Advisor Agentï¼šè§£è¯»å› æœåˆ†æç»“æœï¼Œç”Ÿæˆå†³ç­–å»ºè®®
    
    èŒè´£ï¼š
    1. ç†è§£å› æœåˆ†æçš„æ•°å€¼ç»“æœ
    2. ç”Ÿæˆé€šä¿—æ˜“æ‡‚çš„è§£é‡Š
    3. æä¾›å¯æ“ä½œçš„å·¥è‰ºä¼˜åŒ–å»ºè®®
    """
    print("\n" + "="*80)
    print("ğŸ’¡ Advisor Agent æ­£åœ¨ç”Ÿæˆå†³ç­–å»ºè®®...")
    print("="*80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if state.get('error'):
        return {
            "recommendations": f"åˆ†æè¿‡ç¨‹å‡ºç°é”™è¯¯ï¼Œæ— æ³•ç”Ÿæˆå»ºè®®ã€‚é”™è¯¯ä¿¡æ¯ï¼š{state['error']}"
        }
    
    # ä½¿ç”¨ LLM ç”Ÿæˆå»ºè®®
    llm = ChatOpenAI(
        model=OPENAI_MODEL,
        temperature=0.3,
        openai_api_key=OPENAI_API_KEY,
        base_url=OPENAI_API_BASE
    )
    
    import json
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ··å‡åœŸé…åˆæ¯”ä¼˜åŒ–çš„ä¸“å®¶é¡¾é—®ã€‚è¯·åŸºäºå› æœåˆ†æç»“æœï¼Œç”Ÿæˆå®ç”¨çš„å†³ç­–å»ºè®®ã€‚

åº”ç”¨åœºæ™¯ï¼šé«˜æ€§èƒ½æ··å‡åœŸé…åˆæ¯”è®¾è®¡ä¸å¼ºåº¦ä¼˜åŒ–
æ•°æ®æ¥æºï¼šUCI Machine Learning Repository (Yeh 1998, 1030ä¸ªçœŸå®æ ·æœ¬)

ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

åˆ†æç±»å‹ï¼š{state['analysis_type']}
åˆ†ææ‘˜è¦ï¼š{state['analysis_summary']}

è¯¦ç»†ç»“æœï¼š
{json.dumps(state['causal_results'], indent=2, ensure_ascii=False)}

å…³é”®å˜é‡è¯´æ˜ï¼ˆåŸºäºUCIçœŸå®æ•°æ®é›†ï¼Œ9ä¸ªåŸå§‹å˜é‡ï¼‰ï¼š
- concrete_compressive_strength: æŠ—å‹å¼ºåº¦ï¼ˆMPaï¼‰**ã€ç›®æ ‡å˜é‡ã€‘** - 2.3-82.6ï¼Œå‡å€¼35.8
- cement: æ°´æ³¥ï¼ˆkg/mÂ³ï¼‰**ã€å…³é”®ææ–™ã€‘** - 102-540ï¼Œå‡å€¼281
- blast_furnace_slag: é«˜ç‚‰çŸ¿æ¸£ï¼ˆkg/mÂ³ï¼‰- 0-359ï¼Œå‡å€¼74ï¼Œæ”¹å–„å¯†å®åº¦å’Œè€ä¹…æ€§
- fly_ash: ç²‰ç…¤ç°ï¼ˆkg/mÂ³ï¼‰- 0-200ï¼Œå‡å€¼54ï¼Œç«å±±ç°ååº”ï¼Œé•¿æœŸå¼ºåº¦
- water: æ°´ï¼ˆkg/mÂ³ï¼‰**ã€Abramså®šå¾‹ï¼šæ°´è¶Šå¤šå¼ºåº¦è¶Šä½ã€‘** - 122-247ï¼Œå‡å€¼182
- superplasticizer: é«˜æ•ˆå‡æ°´å‰‚ï¼ˆkg/mÂ³ï¼‰- 0-32ï¼Œå‡å€¼6.2ï¼Œä¸æ°´è´Ÿç›¸å…³ï¼ˆr=-0.66ï¼‰
- coarse_aggregate: ç²—éª¨æ–™ï¼ˆkg/mÂ³ï¼‰- 801-1145ï¼Œå‡å€¼973
- fine_aggregate: ç»†éª¨æ–™ï¼ˆkg/mÂ³ï¼‰- 594-993ï¼Œå‡å€¼774
- age: é¾„æœŸï¼ˆå¤©ï¼‰**ã€æ—¶é—´æ•ˆåº”ã€‘** - 1-365ï¼Œå‡å€¼46

æ°´èƒ¶æ¯”è®¡ç®—å…¬å¼ï¼šwater / (cement + blast_furnace_slag + fly_ash)
æ¨èæ°´èƒ¶æ¯”èŒƒå›´ï¼š0.35-0.50ï¼ˆè¶Šä½å¼ºåº¦è¶Šé«˜ï¼‰

è¯·æä¾›ï¼š
1. **æ ¸å¿ƒå‘ç°**ï¼šç”¨1-2å¥è¯æ€»ç»“æœ€é‡è¦çš„å‘ç°
2. **å…·ä½“å»ºè®®**ï¼šæä¾›3-5æ¡å¯æ“ä½œçš„æ”¹è¿›æªæ–½ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
   - æ˜ç¡®æŒ‡å‡ºåº”è¯¥è°ƒæ•´å“ªä¸ªå‚æ•°
   - è¯´æ˜è°ƒæ•´æ–¹å‘å’Œå¹…åº¦
   - é¢„æœŸæ•ˆæœ
3. **å®æ–½æ–¹æ¡ˆ**ï¼šå»ºè®®çš„å®æ–½é¡ºåºå’Œæ³¨æ„äº‹é¡¹
4. **é£é™©æç¤º**ï¼šå¯èƒ½çš„å‰¯ä½œç”¨æˆ–éœ€è¦ç›‘æ§çš„æŒ‡æ ‡

è¯·ç”¨ä¸“ä¸šä½†é€šä¿—çš„è¯­è¨€ï¼Œç¡®ä¿å»ºè®®å…·æœ‰å¯æ“ä½œæ€§ã€‚
"""
    
    response = llm.invoke(prompt)
    recommendations = response.content
    
    print("\n" + "="*80)
    print("ğŸ“‹ å†³ç­–å»ºè®®")
    print("="*80)
    print(recommendations)
    
    return {
        "recommendations": recommendations
    }


def _generate_analysis_summary(result: dict) -> str:
    """ç”Ÿæˆåˆ†æç»“æœçš„ç®€è¦æ‘˜è¦"""
    analysis_type = result.get('type', 'unknown')
    
    if analysis_type == 'attribution':
        top_factors = result.get('top_factors', [])
        if top_factors:
            top_3 = top_factors[:3]
            summary = f"å½’å› åˆ†æå®Œæˆã€‚ä¸»è¦å½±å“å› ç´ ï¼š" + "ã€".join([
                f"{f['variable']}(è´¡çŒ®{f['contribution']:.4f})" 
                for f in top_3
            ])
        else:
            summary = "å½’å› åˆ†æå®Œæˆï¼Œä½†æœªå‘ç°æ˜¾è‘—å½±å“å› ç´ ã€‚"
            
    elif analysis_type == 'intervention':
        interventions = result.get('interventions', [])
        if interventions:
            top_3 = sorted(interventions, key=lambda x: abs(x['causal_effect']), reverse=True)[:3]
            summary = f"å¹²é¢„åˆ†æå®Œæˆã€‚æœ€æœ‰æ•ˆçš„å¹²é¢„æªæ–½ï¼š" + "ã€".join([
                f"{i['variable']}(æ•ˆåº”{i['causal_effect']:.4f})"
                for i in top_3
            ])
        else:
            summary = "å¹²é¢„åˆ†æå®Œæˆï¼Œä½†æœªå‘ç°æœ‰æ•ˆçš„å¹²é¢„æªæ–½ã€‚"
            
    elif analysis_type == 'counterfactual':
        effect = result.get('causal_effect', 0)
        interventions_list = result.get('interventions', [])
        
        if len(interventions_list) == 1:
            # å•å˜é‡å¹²é¢„
            interv = interventions_list[0]
            summary = f"åäº‹å®åˆ†æå®Œæˆã€‚å¦‚æœ{interv['variable']}ä»" \
                      f"{interv['original_value']:.4f}æ”¹ä¸º{interv['new_value']:.4f}ï¼Œ" \
                      f"{result['target']}é¢„æœŸå˜åŒ–{effect:.4f}ã€‚"
        else:
            # å¤šå˜é‡å¹²é¢„
            interv_desc = "ã€".join([
                f"{i['variable']}={i['new_value']:.2f}" 
                for i in interventions_list
            ])
            summary = f"åäº‹å®åˆ†æå®Œæˆã€‚å¦‚æœå¹²é¢„{interv_desc}ï¼Œ" \
                      f"{result['target']}é¢„æœŸå˜åŒ–{effect:.4f}ã€‚"
    else:
        summary = "åˆ†æå®Œæˆã€‚"
    
    return summary


# ============================================================================
# ç¬¬å››æ­¥ï¼šæ„å»º LangGraph Workflow
# ============================================================================

def create_causal_agent_graph():
    """åˆ›å»ºå› æœåˆ†ææ™ºèƒ½ä½“å·¥ä½œæµ"""
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(CausalAnalysisState)
    
    # æ·»åŠ ä¸‰ä¸ªæ™ºèƒ½ä½“èŠ‚ç‚¹
    workflow.add_node("router", router_agent)
    workflow.add_node("analyst", causal_analyst_agent)
    workflow.add_node("advisor", advisor_agent)
    
    # å®šä¹‰æµç¨‹ï¼šSTART â†’ Router â†’ Analyst â†’ Advisor â†’ END
    workflow.add_edge(START, "router")
    workflow.add_edge("router", "analyst")
    workflow.add_edge("analyst", "advisor")
    workflow.add_edge("advisor", END)
    
    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile()
    
    return app


# ============================================================================
# å¯¼å‡ºæ¥å£
# ============================================================================

__all__ = [
    'CausalAnalysisState',
    'initialize_causal_model',
    'create_causal_agent_graph',
    'router_agent',
    'causal_analyst_agent',
    'advisor_agent'
]

